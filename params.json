{
  "name": "Rbfn two mnist",
  "tagline": "Radial basis function network for two classes of the MNIST dataset",
  "body": "### A Radial basis function network\r\nIn this blog-post, we will discuss radial basis function networks(RBFN). RBNF's seem to have lost the race for popularity as compared to Neural Network. Yet, they prove a powerful generalization capacity. RBFN's are similar to NN in the sense that they use multiple layers of perceptrons to define a function over the input space. Yet, in an RBFN, the first layer generalizes the input space. That is, the input layer makes use of the distances between fixed center points and input data. The consequent activation functions are radially symmetric. Hence the name, radial basis function networks.\r\n\r\n### Basic implementation\r\nThis code is a mere proof of concept We explore the world of RBFN with a basic implementation. The dataset are two selected classes from the MNIST dataset. We implement both exact interpolation and approximate interpolation:\r\n* In exact interpolation, we have as much hidden neurons in the firsy layer as we have train samples. Moreover, we pick the center points of the hidden neurons to be exactly these train samples. Obviously, we are overfitting in many aspects. \r\n* In approximate interpolation, we pick less hidden neurons. This is a hyperparameter. Still, the center points of the hidden neurons are picked to be train samples.\r\n\r\n### Evaluation\r\nRBFN's have no deterministic threshold, even when the targets are 0 and 1. Therefore, we evaluate the accuracies over a range of possible thresholds:\r\n![Accuracies for RBFN on  two-class MNIST](https://github.com/RobRomijnders/RBFN_two_MNIST/blob/master/explore_threshold.png?raw=true)\r\n\r\nAlso interesting is to vary the regularization on the final perceptron or the variance of the radial basis function:\r\n\r\n![Explore different regularizations for RBFN on two-class MNIST](https://github.com/RobRomijnders/RBFN_two_MNIST/blob/master/explore_regularization.png)\r\n\r\n![Explore different variances of RBF for RBFN on two-class MNIST](https://github.com/RobRomijnders/RBFN_two_MNIST/blob/master/explore_deviation.png)\r\n\r\n### Further improvements\r\nThis implementation of an RBFN is basic. Anyone who would like to take this further can consider\r\n* Optimize the variance of the RBF. There are many articles and bookchapters on optimizing the variance of the RBF. Many implementations treat the variance as an input parameter and back-propagate into it.\r\n* Back-propagate to the centerpoints. We took the location of the center points as random train samples. Another possibility is to treat the center point as an input parameter as well and back-propagate into it.\r\n* Add an new hidden layer to the RBFN. Learning with back-propagation also allows you to add hidden layers to your liking. Now the radial basis neurons improve the generalization and the multiple perceptrons improve the versatility. Together they will provide a powerful model.\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}